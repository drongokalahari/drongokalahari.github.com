---
title: "Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning"
categories:
  - Project Paper Review
tags:
  - paper
  - USENIX
---

# General Info
- USENIX 2020
- [Link](https://arxiv.org/pdf/1801.08917.pdf)

## Core Contents

## What is it?
However, motivated and sophisticated adversaries intentionally seek to evade antivirus engines, be they signature-based or otherwise. 
* In the context of a machine learning model, an attacker’s aim is to discover and exploit a set of features that the model deems discriminating, but may not be a causal indicator of benign behavior.

## Goal
In this paper, we present a reinforcement learning (RL) approach to learn to bypass machine learning antivirus models based on static features. 
Static detection of malware is an important protection layer in security suites because it allows malicious files to be detected prior to execution. 
Our intent in attacking machine learning malware models is two-fold: 
* to provide an automated framework to summarize the weaknesses of an anti-malware engine, and 
* to produce functioning evasive malware samples that can be used to augment a machine learning model in adversarial training.

## How to?
In our experiments, we train an ACER agent to learn a policy for our framework depicted in Figure 1. In the Markov decision process shown, the agent gets an estimate of the environment’s state s ∈ S, represented by a feature vector s of the malware sample (which need not correspond to any internal representation of the malware by the anti-malware engine). 
* The Q-function and action policy determine what action to take. 
* In our framework, the actions space A consists of a set of modifications to the PE file that - 
  * (a) do not break the PE file format, and 
  * (b) do not alter the intended functionality of the malware sample. 
* **The reward function is measured by the antimalware engine, which is converted to a reward: 0 if the modified malware sample is judged to be malicious (no evasion), and R if it is deemed to be benign (evasion).**
* The reward and state are then fed back into the agent.

## Conclusion
Works well! :)

## My review
Interesting.
I see enormous effors put in this work.
So many feature spaces and trying to do adversarial learning to show its effectiveness in the opposite aspect...
- Q. Reward function is binary? just pass and fail from anti-virus engine?